{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 在Domain A上训练的图像识别模型, 在Domain B上测试\n### simple baseline(lambda=0.1): 51.7%\n### 把网络改轻一点: 29%,sb\n### 原始网络,feature输出层激活函数换tanh,Classifier加BN,lambda=0.3,500epochs: 32.7%\n### 最后的Classifier不加BN: 54%\n### 小模型不带BN, lambda=0.3: 57.3%\n### lambda=1, 爆train 500epochs: 49.9%\n### lambda=3, epoch=200: 51.7%\n### lambda用schedule(0->2), canny(200, 300), epoch=400,大模型: 50%\n### 复刻一下小模型再用在聚类上","metadata":{}},{"cell_type":"markdown","source":"### 训练集图片","metadata":{}},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\ndef no_axis_show(img, title='', cmap=None):\n    # imshow, and set the interpolation mode to be \"nearest\"。\n    fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n    # do not show the axes in the images.\n    fig.axes.get_xaxis().set_visible(False)\n    fig.axes.get_yaxis().set_visible(False)\n    plt.title(title)\n\ntitles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\nplt.figure(figsize=(18, 18))\nfor i in range(10):\n    plt.subplot(1, 10, i+1)\n    fig = no_axis_show(plt.imread(f'../input/ml2021spring-hw11/real_or_drawing/train_data/{i}/{500*i}.bmp'), title=titles[i])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:24.523465Z","iopub.execute_input":"2022-06-02T08:16:24.524578Z","iopub.status.idle":"2022-06-02T08:16:25.564507Z","shell.execute_reply.started":"2022-06-02T08:16:24.524432Z","shell.execute_reply":"2022-06-02T08:16:25.563633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 测试集图片","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18, 18))\nfor i in range(10):\n    plt.subplot(1, 10, i+1)\n    fig = no_axis_show(plt.imread(f'../input/ml2021spring-hw11/real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:38:23.203643Z","iopub.execute_input":"2022-06-02T03:38:23.204177Z","iopub.status.idle":"2022-06-02T03:38:23.765029Z","shell.execute_reply.started":"2022-06-02T03:38:23.204137Z","shell.execute_reply":"2022-06-02T03:38:23.76421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 创建dataset(用canny做边缘检测把RGB的图变成灰度图)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\n\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\n\nsource_transform = transforms.Compose([\n    # Turn RGB to grayscale. (Bacause Canny do not support RGB images.)\n    transforms.Grayscale(),\n    # cv2 do not support skimage.Image, so we transform it to np.array, \n    # and then adopt cv2.Canny algorithm.\n    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n    # Transform np.array back to the skimage.Image.\n    transforms.ToPILImage(),\n    # 50% Horizontal Flip. (For Augmentation)\n    transforms.RandomHorizontalFlip(),\n    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n    # if there's empty pixel after rotation.\n    transforms.RandomRotation(15, fill=(0,)),\n    # Transform to tensor for model inputs.\n    transforms.ToTensor(),\n])\ntarget_transform = transforms.Compose([\n    # Turn RGB to grayscale.\n    transforms.Grayscale(),\n    # Resize: size of source data is 32x32, thus we need to \n    #  enlarge the size of target data from 28x28 to 32x32。\n    transforms.Resize((32, 32)),\n    # 50% Horizontal Flip. (For Augmentation)\n    transforms.RandomHorizontalFlip(),\n    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n    # if there's empty pixel after rotation.\n    transforms.RandomRotation(15, fill=(0,)),\n    # Transform to tensor for model inputs.\n    transforms.ToTensor(),\n])\n\nsource_dataset = ImageFolder('../input/ml2021spring-hw11/real_or_drawing/train_data', transform=source_transform)\ntarget_dataset = ImageFolder('../input/ml2021spring-hw11/real_or_drawing/test_data', transform=target_transform)\n\nsource_dataloader = DataLoader(source_dataset, batch_size=256, shuffle=True)\ntarget_dataloader = DataLoader(target_dataset, batch_size=1024, shuffle=True)\ntest_dataloader = DataLoader(target_dataset, batch_size=1024, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:29.030618Z","iopub.execute_input":"2022-06-02T08:16:29.031029Z","iopub.status.idle":"2022-06-02T08:17:51.447233Z","shell.execute_reply.started":"2022-06-02T08:16:29.030995Z","shell.execute_reply":"2022-06-02T08:17:51.446533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 模型","metadata":{}},{"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 64, 3, 1, 1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(64, 128, 3, 1, 1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(128, 256, 3, 1, 1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(256, 256, 3, 1, 1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(256, 512, 3, 1, 1),\n            nn.BatchNorm2d(512),\n            nn.Tanh(),\n            nn.MaxPool2d(2)\n        )\n        \n    def forward(self, x):\n        x = self.conv(x).squeeze()\n        return x\n\nclass LabelPredictor(nn.Module):\n\n    def __init__(self):\n        super(LabelPredictor, self).__init__()\n\n        self.layer = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.ReLU(),\n\n            nn.Linear(512, 512),\n            nn.ReLU(),\n\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, h):\n        c = self.layer(h)\n        return c\n\nclass DomainClassifier(nn.Module):\n\n    def __init__(self):\n        super(DomainClassifier, self).__init__()\n\n        self.layer = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, h):\n        y = self.layer(h)\n        return y","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:17:51.449135Z","iopub.execute_input":"2022-06-02T08:17:51.450018Z","iopub.status.idle":"2022-06-02T08:17:51.46812Z","shell.execute_reply.started":"2022-06-02T08:17:51.449974Z","shell.execute_reply":"2022-06-02T08:17:51.467205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 小模型","metadata":{}},{"cell_type":"code","source":"# class Test_model(nn.Module):\n\n#     def __init__(self):\n#         super(Test_model, self).__init__()\n\n#         self.conv = nn.Sequential(\n#             nn.Conv2d(1, 16, 3, 1, 1),\n#             nn.BatchNorm2d(16),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),\n\n#             nn.Conv2d(16, 32, 3, 1, 1),\n#             nn.BatchNorm2d(32),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),\n\n#             nn.Conv2d(32, 64, 3, 1, 1),\n#             nn.BatchNorm2d(64),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),\n\n#             nn.Conv2d(64, 64, 3, 1, 1),\n#             nn.BatchNorm2d(64),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),\n\n#             nn.Conv2d(64, 128, 3, 1, 1),\n#             nn.BatchNorm2d(128),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),\n            \n#             nn.Linear(128, 10),\n#         )\n        \n#     def forward(self, x):\n#         x = self.conv(x)\n#         return x\n\n\n# class FeatureExtractor(nn.Module):\n\n#     def __init__(self):\n#         super(FeatureExtractor, self).__init__()\n\n#         self.conv = nn.Sequential(\n#             nn.Conv2d(1, 16, 3, 1, 1),\n#             nn.BatchNorm2d(16),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),\n\n#             nn.Conv2d(16, 32, 3, 1, 1),\n#             nn.BatchNorm2d(32),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),\n\n#             nn.Conv2d(32, 64, 3, 1, 1),\n#             nn.BatchNorm2d(64),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),\n\n#             nn.Conv2d(64, 64, 3, 1, 1),\n#             nn.BatchNorm2d(64),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),\n\n#             nn.Conv2d(64, 128, 3, 1, 1),\n#             nn.BatchNorm2d(128),\n#             nn.Tanh(),\n#             nn.MaxPool2d(2)\n#         )\n        \n#     def forward(self, x):\n#         x = self.conv(x).squeeze()\n#         return x\n\n# class LabelPredictor(nn.Module):\n\n#     def __init__(self):\n#         super(LabelPredictor, self).__init__()\n\n#         self.layer = nn.Sequential(\n#             nn.Linear(128, 32),\n#             nn.ReLU(),\n#             nn.Linear(32, 10),\n#         )\n\n#     def forward(self, h):\n#         c = self.layer(h)\n#         return c\n\n# class DomainClassifier(nn.Module):\n\n#     def __init__(self):\n#         super(DomainClassifier, self).__init__()\n\n#         self.layer = nn.Sequential(\n#             nn.Linear(128, 128),\n#             nn.BatchNorm1d(128),\n#             nn.ReLU(),\n\n#             nn.Linear(128, 32),\n#             nn.BatchNorm1d(32),\n#             nn.ReLU(),\n\n#             nn.Linear(32, 1),\n#         )\n\n#     def forward(self, h):\n#         y = self.layer(h)\n#         return y","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:38:38.102041Z","iopub.execute_input":"2022-06-02T03:38:38.102527Z","iopub.status.idle":"2022-06-02T03:38:38.115462Z","shell.execute_reply.started":"2022-06-02T03:38:38.102492Z","shell.execute_reply":"2022-06-02T03:38:38.11463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extractor = FeatureExtractor().cuda()\nlabel_predictor = LabelPredictor().cuda()\ndomain_classifier = DomainClassifier().cuda()\n\nclass_criterion = nn.CrossEntropyLoss()\ndomain_criterion = nn.BCEWithLogitsLoss()\n\nlearning_rate = 1e-3\nnum_epochs = 300\nL = 0.3  # L:lambda\n\noptimizer_F = optim.Adam(feature_extractor.parameters(), lr=learning_rate)\noptimizer_C = optim.Adam(label_predictor.parameters(), lr=learning_rate)\noptimizer_D = optim.Adam(domain_classifier.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:38:38.116924Z","iopub.execute_input":"2022-06-02T03:38:38.117565Z","iopub.status.idle":"2022-06-02T03:38:38.163688Z","shell.execute_reply.started":"2022-06-02T03:38:38.117528Z","shell.execute_reply":"2022-06-02T03:38:38.162995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### semi-supervise","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nclass MyDataset(Dataset):              # 自制数据集,继承Dataset,用来生成batch\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __getitem__(self, index):           # 返回的是tensor\n        x, y = self.x[index], self.y[index]\n        return x, y\n\n    def __len__(self):\n        return len(self.x)\n\ndef get_pseudo_labels(dataset, model1, model2, threshold=0.1, batch_size = 1024):\n    # This functions generates pseudo-labels of a dataset using given model.\n    # It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.\n    # You are NOT allowed to use any models trained on external data for pseudo-labeling.\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Construct a data loader.\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    # Make sure the model is in eval mode.\n    model1.eval()\n    model2.eval()\n    # Define softmax function.\n    softmax = nn.Softmax(dim=-1)\n\n    # Iterate over the dataset by batches.\n    data_x = torch.tensor([])\n    data_y = torch.tensor([])\n    for batch in data_loader:\n        img, _ = batch\n        # Forward the data\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = model2(model1(img.to(device)))\n\n        # Obtain the probability distributions by applying softmax on logits.\n        probs = softmax(logits)     # (n_b, 11)\n        x, y = torch.max(probs, dim=1)\n        y = y.cpu()\n        idx = x > threshold\n        data_x = torch.cat([data_x, img[idx]])\n        data_y = torch.cat([data_y, y[idx]])\n        # ---------- TODO ----------\n        # Filter the data and construct a new dataset.\n    if len(data_x) == 0:\n        return None\n    new_dataset = MyDataset(data_x, data_y)\n    # # Turn off the eval mode.\n    model1.train()\n    model2.train()\n    return new_dataset","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:38:38.164872Z","iopub.execute_input":"2022-06-02T03:38:38.165213Z","iopub.status.idle":"2022-06-02T03:38:38.178539Z","shell.execute_reply.started":"2022-06-02T03:38:38.165179Z","shell.execute_reply":"2022-06-02T03:38:38.177385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 训练(可调lambda)\n### D: domain classifier\n### F: feature extractor\n### C: label predictor","metadata":{}},{"cell_type":"code","source":"def train_epoch(source_dataloader, target_dataloader, lamb):\n    '''\n      Args:\n        source_dataloader: source data的dataloader\n        target_dataloader: target data的dataloader\n        lamb: control the balance of domain adaptatoin and classification.\n    '''\n\n    # D loss: Domain Classifier的loss\n    # F loss: Feature Extrator & Label Predictor的loss\n    running_D_loss, running_F_loss = 0.0, 0.0\n    total_hit, total_num = 0.0, 0.0\n    # 加个Discriminator的准确率\n    total_dis, total_dis_num = 0.0, 0.0\n    \n\n    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n\n        source_data = source_data.cuda()\n        source_label = source_label.cuda()\n        target_data = target_data.cuda()\n        \n        # Mixed the source data and target data, or it'll mislead the running params\n        #   of batch_norm. (runnning mean/var of soucre and target data are different.)\n        mixed_data = torch.cat([source_data, target_data], dim=0)\n        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n        # set domain label of source data to be 1.\n        domain_label[:source_data.shape[0]] = 1\n\n        # Step 1 : train domain classifier\n        feature = feature_extractor(mixed_data)\n        # We don't need to train feature extractor in step 1.\n        # Thus we detach the feature neuron to avoid backpropgation.\n        # 1.训练Discriminator\n        # 在feature这里detach斩断了梯度传导,这样只会改变domain_classifier的参数而不会改变前面generator的参数\n        domain_logits = domain_classifier(feature.detach())\n        loss = domain_criterion(domain_logits, domain_label)\n        running_D_loss+= loss.item()\n        loss.backward()\n        optimizer_D.step()\n\n        # Step 2 : train feature extractor and label classifier\n        # 2.训练以特征为输入的分类器(C)+特征抽取的Generator(F)\n        class_logits = label_predictor(feature[:source_data.shape[0]])\n        domain_logits = domain_classifier(feature)\n        # loss = cross entropy of classification - lamb * domain binary cross entropy.\n        #  The reason why using subtraction is similar to generator loss in disciminator of GAN\n        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n        \n        running_F_loss+= loss.item()\n        loss.backward()\n        optimizer_F.step()\n        optimizer_C.step()\n\n        optimizer_D.zero_grad()\n        optimizer_F.zero_grad()\n        optimizer_C.zero_grad()\n\n        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n        total_num += source_data.shape[0]\n        total_dis += torch.sum((domain_logits > 0.5) == domain_label).item()\n        total_dis_num += domain_label.shape[0]\n#         print(i, end='\\r')\n\n    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num, total_dis / total_dis_num\n\ntest_model = Test_model().to('cuda')\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(test_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n# train 200 epochs\nfor epoch in range(num_epochs):\n    # 训练test_model\n#     if epoch > num_epochs//2:  # semi-supervise\n#     if epoch >= 0:  # semi-supervise\n#         pseudo_set = get_pseudo_labels(target_dataset, model1=feature_extractor, model2=label_predictor)\n#         if pseudo_set != None:\n#             print('pseudo长度: ', len(pseudo_set))\n#             pseudo_loader = DataLoader(pseudo_set, batch_size=1024, shuffle=True)\n#             test_model.train()\n#             for batch in pseudo_loader:           # unlabel标签训练\n#                 imgs, labels = batch\n#                 logits = test_model(imgs.to('cuda'))\n#                 loss = loss_func(logits, labels.long().to(device))\n#                 optimizer.zero_grad()\n#                 loss.backward()\n#                 grad_norm = nn.utils.clip_grad_norm_(test_model.parameters(), max_norm=10)\n#                 optimizer.step()\n        \n    \n    # You should chooose lamnda cleverly.\n#     cur_lamb = L*((epoch/num_epochs)**(1/3))\n    train_D_loss, train_F_loss, train_acc, dis_acc = train_epoch(source_dataloader, target_dataloader, lamb=L)\n\n    torch.save(feature_extractor.state_dict(), f'extractor_model.bin')\n    torch.save(label_predictor.state_dict(), f'predictor_model.bin')\n    print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, Classifier acc: {:6.4f}, dis_acc: {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc, dis_acc))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:38:38.181047Z","iopub.execute_input":"2022-06-02T03:38:38.181629Z","iopub.status.idle":"2022-06-02T05:53:15.97649Z","shell.execute_reply.started":"2022-06-02T03:38:38.181586Z","shell.execute_reply":"2022-06-02T05:53:15.975547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"result = []\nlabel_predictor.eval()\nfeature_extractor.eval()\nfor i, (test_data, _) in enumerate(test_dataloader):\n    test_data = test_data.cuda()\n\n    class_logits = label_predictor(feature_extractor(test_data))\n#     class_logits = test_model(test_data)  # semi-supervise\n\n    x = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n    result.append(x)\n\nimport pandas as pd\nresult = np.concatenate(result)\n\n# Generate your submission\ndf = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\ndf.to_csv('DaNN_submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:53:15.978008Z","iopub.execute_input":"2022-06-02T05:53:15.978956Z","iopub.status.idle":"2022-06-02T05:54:53.387101Z","shell.execute_reply.started":"2022-06-02T05:53:15.978916Z","shell.execute_reply":"2022-06-02T05:54:53.386236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ntime.sleep(40000)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:54:53.389164Z","iopub.execute_input":"2022-06-02T05:54:53.389507Z"},"trusted":true},"execution_count":null,"outputs":[]}]}