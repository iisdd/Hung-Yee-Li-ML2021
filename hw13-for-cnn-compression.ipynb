{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 记录\n### Simple baseline: 重train一个小model (52.4%)\n### 换KL loss训练30epochs (59.9%)\n### 用分离的卷积block,lr=1e-4,135epochs (68.2%,过medium)\n### 把KD loss的p改成log_softmax激活 ","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-08T07:10:55.49557Z","iopub.execute_input":"2022-06-08T07:10:55.49601Z","iopub.status.idle":"2022-06-08T07:11:06.865099Z","shell.execute_reply.started":"2022-06-08T07:10:55.4959Z","shell.execute_reply":"2022-06-08T07:11:06.863997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset # \"ConcatDataset\" and \"Subset\" are possibly useful.\nfrom torchvision.datasets import DatasetFolder, VisionDataset\nfrom torchsummary import summary\nfrom tqdm.auto import tqdm\nimport random\nimport torchvision","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:11:06.867306Z","iopub.execute_input":"2022-06-08T07:11:06.867913Z","iopub.status.idle":"2022-06-08T07:11:09.020795Z","shell.execute_reply.started":"2022-06-08T07:11:06.867872Z","shell.execute_reply":"2022-06-08T07:11:09.020012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"cfg = {\n    'dataset_root': '/kaggle/input/ml2022spring-hw13/food11-hw13',\n    'save_dir': '/kaggle/working/outputs',\n    'exp_name': \"simple_baseline\",\n    'batch_size': 128,\n    'lr': 3e-4,\n    'seed': 20220013,\n    'loss_fn_type': 'KD', # simple baseline: CE, medium baseline: KD. See the Knowledge_Distillation part for more information.\n    'weight_decay': 1e-5,\n    'grad_norm_max': 10,\n    'n_epochs': 200,\n    'patience': 10,    # early stop用的,如果10个epoch Acc没上升就停\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:11:34.438915Z","iopub.execute_input":"2022-06-08T07:11:34.439479Z","iopub.status.idle":"2022-06-08T07:11:34.44502Z","shell.execute_reply.started":"2022-06-08T07:11:34.439448Z","shell.execute_reply":"2022-06-08T07:11:34.443846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myseed = cfg['seed']  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nrandom.seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)\n\nsave_path = os.path.join(cfg['save_dir'], cfg['exp_name']) # create saving directory\nos.makedirs(save_path, exist_ok=True)\n\n# define simple logging functionality\nlog_fw = open(f\"{save_path}/log.txt\", 'w') # open log file to save log outputs\ndef log(text):     # define a logging function to trace the training process\n    print(text)\n    log_fw.write(str(text)+'\\n')\n    log_fw.flush()\n\nlog(cfg)  # log your configs to the log file","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:11:38.395064Z","iopub.execute_input":"2022-06-08T07:11:38.395588Z","iopub.status.idle":"2022-06-08T07:11:38.468438Z","shell.execute_reply.started":"2022-06-08T07:11:38.395556Z","shell.execute_reply":"2022-06-08T07:11:38.467633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset(food11)","metadata":{}},{"cell_type":"code","source":"# 打印各文件夹里图片数量\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    if len(filenames) > 0:\n        print(f\"{dirname}: {len(filenames)} files.\")","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:11:40.752555Z","iopub.execute_input":"2022-06-08T07:11:40.753204Z","iopub.status.idle":"2022-06-08T07:11:46.09254Z","shell.execute_reply.started":"2022-06-08T07:11:40.75317Z","shell.execute_reply":"2022-06-08T07:11:46.09164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### transform(可照抄HW3)","metadata":{}},{"cell_type":"code","source":"# 这个normalize是teacher model用的\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n# define training/testing transforms\ntest_tfm = transforms.Compose([\n    # It is not encouraged to modify this part if you are using the provided teacher model. This transform is stardard and good enough for testing.\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    normalize,\n])\n\ntrain_tfm = transforms.Compose([\n    # add some useful transform or augmentation here, according to your experience in HW3.\n#     transforms.Resize(256),  # You can change this\n#     transforms.CenterCrop(224), # You can change this, but be aware of that the given teacher model's input size is 224.\n    \n    torchvision.transforms.RandomResizedCrop((224, 224), scale=(0.3, 1.0), ratio=(0.75, 1.333)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(30, expand=False, center=None),\n    \n    # The training input size of the provided teacher model is (3, 224, 224).\n    # Thus, Input size other then 224 might hurt the performance. please be careful.\n    transforms.RandomHorizontalFlip(), # You can change this.\n    transforms.ToTensor(),\n    normalize,\n])","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:11:46.094418Z","iopub.execute_input":"2022-06-08T07:11:46.094913Z","iopub.status.idle":"2022-06-08T07:11:46.104259Z","shell.execute_reply.started":"2022-06-08T07:11:46.094875Z","shell.execute_reply":"2022-06-08T07:11:46.10333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FoodDataset(Dataset):\n    def __init__(self, path, tfm=test_tfm, files = None):\n        super().__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n        print(f\"One {path} sample\",self.files[0])\n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n        return im,label","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:11:48.789837Z","iopub.execute_input":"2022-06-08T07:11:48.790629Z","iopub.status.idle":"2022-06-08T07:11:48.800181Z","shell.execute_reply.started":"2022-06-08T07:11:48.790594Z","shell.execute_reply":"2022-06-08T07:11:48.799226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = FoodDataset(os.path.join(cfg['dataset_root'],\"training\"), tfm=train_tfm)\ntrain_loader = DataLoader(train_set, batch_size=cfg['batch_size'], shuffle=True, num_workers=0, pin_memory=True)\n\nvalid_set = FoodDataset(os.path.join(cfg['dataset_root'], \"validation\"), tfm=test_tfm)\nvalid_loader = DataLoader(valid_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:11:51.340876Z","iopub.execute_input":"2022-06-08T07:11:51.341657Z","iopub.status.idle":"2022-06-08T07:11:51.382454Z","shell.execute_reply.started":"2022-06-08T07:11:51.341623Z","shell.execute_reply":"2022-06-08T07:11:51.381567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 用Depthwise&Pointwise代替卷积\n### 参数量: IOKK -> IKK+IO","metadata":{}},{"cell_type":"code","source":"def dwpw_conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, groups=in_channels), #depthwise convolution\n        nn.BatchNorm2d(in_channels),\n        nn.ReLU(),\n        nn.Conv2d(in_channels, out_channels, 1), # pointwise convolution\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n    )","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:11:53.771356Z","iopub.execute_input":"2022-06-08T07:11:53.77208Z","iopub.status.idle":"2022-06-08T07:11:53.777674Z","shell.execute_reply.started":"2022-06-08T07:11:53.772044Z","shell.execute_reply":"2022-06-08T07:11:53.776858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define your student network here. You have to copy-paste this code block to HW13 GradeScope before deadline.\n# We will use your student network definition you submit to evaluate your results(including the total parameter amount).\n\nclass StudentNet(nn.Module):\n    def __init__(self):\n      super().__init__()\n\n      # ---------- TODO ----------\n      # Modify your model architecture\n\n      self.cnn = nn.Sequential(\n        dwpw_conv(3, 32, 3, padding=1), \n        dwpw_conv(32, 32, 3, padding=1),\n        nn.MaxPool2d(2, 2, 0),     \n        dwpw_conv(32, 64, 3, padding=1),\n        nn.MaxPool2d(2, 2, 0),     \n        dwpw_conv(64, 64, 3, padding=1),\n        dwpw_conv(64, 128, 3, padding=1),\n        nn.MaxPool2d(2, 2, 0),\n        dwpw_conv(128, 128, 3, padding=1),\n        nn.MaxPool2d(2, 2, 0),\n        dwpw_conv(128, 256, 3, padding=1),\n        nn.MaxPool2d(2, 2, 0),\n          \n        # Here we adopt Global Average Pooling for various input size.\n        nn.AdaptiveAvgPool2d((1, 1)),  # 自适应平均池化\n      )\n      self.fc = nn.Sequential(\n        nn.Linear(256, 11),\n      )\n      \n    def forward(self, x):\n      out = self.cnn(x)\n      out = out.view(out.size()[0], -1)\n      return self.fc(out)\n\ndef get_student_model(): # This function should have no arguments so that we can get your student network by directly calling it.\n    # you can modify or do anything here, just remember to return an nn.Module as your student network.  \n    return StudentNet() \n\n# End of definition of your student model and the get_student_model API\n# Please copy-paste the whole code block, including the get_student_model function.","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:11:54.980966Z","iopub.execute_input":"2022-06-08T07:11:54.981912Z","iopub.status.idle":"2022-06-08T07:11:54.993435Z","shell.execute_reply.started":"2022-06-08T07:11:54.981874Z","shell.execute_reply":"2022-06-08T07:11:54.992357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 看看网络结构(参数量<100k)","metadata":{}},{"cell_type":"code","source":"# This is how we will evaluate your student model.\n# Do not modify this code block.\nstudent_model = get_student_model()\nsummary(student_model, (3, 224, 224), device='cpu')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:11:56.336807Z","iopub.execute_input":"2022-06-08T07:11:56.337303Z","iopub.status.idle":"2022-06-08T07:11:56.743534Z","shell.execute_reply.started":"2022-06-08T07:11:56.337269Z","shell.execute_reply":"2022-06-08T07:11:56.741947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 加载teacher network","metadata":{}},{"cell_type":"code","source":"# Load provided teacher model (model architecture: resnet18, num_classes=11, test-acc ~= 89.9%)\nteacher_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=11)\n# load state dict\nteacher_ckpt_path = os.path.join(cfg['dataset_root'], \"resnet18_teacher.ckpt\")\nteacher_model.load_state_dict(torch.load(teacher_ckpt_path, map_location='cpu'))\n# Now you already know the teacher model's architecture. You can take advantage of it if you want to pass the strong or boss baseline. \n# Source code of resnet in pytorch: (https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py)\n# You can also see the summary of teacher model. There are 11,182,155 parameters totally in the teacher model\nsummary(teacher_model, (3, 224, 224), device='cpu')\nteacher_model = teacher_model.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:12:01.852714Z","iopub.execute_input":"2022-06-08T07:12:01.853234Z","iopub.status.idle":"2022-06-08T07:12:08.470771Z","shell.execute_reply.started":"2022-06-08T07:12:01.853198Z","shell.execute_reply":"2022-06-08T07:12:08.469908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 用knowledge distillation的loss","metadata":{}},{"cell_type":"code","source":"# Implement the loss function with KL divergence loss for knowledge distillation.\n# You also have to copy-paste this whole block to HW13 GradeScope. \ndef loss_fn_kd(student_logits, labels, teacher_logits, alpha=0.6, temperature=1.0):\n    # ------------TODO-------------\n    KL = nn.KLDivLoss(reduction=\"batchmean\")\n    CE = nn.CrossEntropyLoss()\n    Softmax = nn.Softmax()\n    p = Softmax(student_logits/temperature)\n    q = nn.log_softmax(teacher_logits/temperature)  # 这里要加log不然loss会变成负数\n    Loss = alpha*(temperature**2)*KL(p, q) + (1-alpha)*CE(student_logits, labels)\n    # Refer to the above formula and finish the loss function for knowkedge distillation using KL divergence loss and CE loss.\n    # If you have no idea, please take a look at the provided useful link above.\n    return Loss","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:12:08.472467Z","iopub.execute_input":"2022-06-08T07:12:08.472843Z","iopub.status.idle":"2022-06-08T07:12:08.47914Z","shell.execute_reply.started":"2022-06-08T07:12:08.472806Z","shell.execute_reply":"2022-06-08T07:12:08.478198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# choose the loss function by the config\nif cfg['loss_fn_type'] == 'CE':\n    # For the classification task, we use cross-entropy as the default loss function.\n    loss_fn = nn.CrossEntropyLoss() # loss function for simple baseline.\n\nif cfg['loss_fn_type'] == 'KD': # KD stands for knowledge distillation\n    loss_fn = loss_fn_kd # implement loss_fn_kd for the report question and the medium baseline.\n\n# You can also other types of knowledge distillation techniques, but use function name other than `loss_fn_kd`\n# For example:\n# def loss_fn_custom_kd():\n#     pass\n# if cfg['loss_fn_type'] == 'custom_kd':\n#     loss_fn = loss_fn_custom_kd\n\n# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nlog(f\"device: {device}\")\n\n# The number of training epochs and patience.\nn_epochs = cfg['n_epochs']\npatience = cfg['patience'] # If no improvement in 'patience' epochs, early stop","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:12:11.216861Z","iopub.execute_input":"2022-06-08T07:12:11.217318Z","iopub.status.idle":"2022-06-08T07:12:11.22921Z","shell.execute_reply.started":"2022-06-08T07:12:11.21728Z","shell.execute_reply":"2022-06-08T07:12:11.227919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 训练","metadata":{}},{"cell_type":"code","source":"# Initialize a model, and put it on the device specified.\nstudent_model.to(device)\n# teacher_model.to(device) # MEDIUM BASELINE\n\n# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\noptimizer = torch.optim.Adam(student_model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay']) \n\n# Initialize trackers, these are not parameters and should not be changed\nstale = 0\nbest_acc = 0.0\n\n# teacher_model.eval()  # MEDIUM BASELINE\nfor epoch in range(n_epochs):\n\n    # ---------- Training ----------\n    # Make sure the model is in train mode before training.\n    student_model.train()\n\n    # These are used to record information in training.\n    train_loss = []\n    train_accs = []\n    train_lens = []\n\n    for batch in tqdm(train_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n        #imgs = imgs.half()\n        #print(imgs.shape,labels.shape)\n\n        # Forward the data. (Make sure data and model are on the same device.)\n        with torch.no_grad():  # MEDIUM BASELINE\n            teacher_logits = teacher_model(imgs)  # MEDIUM BASELINE\n        \n        logits = student_model(imgs)\n\n        # Calculate the cross-entropy loss.\n        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n        loss = loss_fn(logits, labels, teacher_logits) # MEDIUM BASELINE\n#         loss = loss_fn(logits, labels) # SIMPLE BASELINE\n        # Gradients stored in the parameters in the previous step should be cleared out first.\n        optimizer.zero_grad()\n\n        # Compute the gradients for parameters.\n        loss.backward()\n\n        # Clip the gradient norms for stable training.\n        # 防止梯度爆炸,不防梯度弥散\n        grad_norm = nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=cfg['grad_norm_max'])\n\n        # Update the parameters with computed gradients.\n        optimizer.step()\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels).float().sum()\n\n        # Record the loss and accuracy.\n        train_batch_len = len(imgs)\n        train_loss.append(loss.item() * train_batch_len)\n        train_accs.append(acc)\n        train_lens.append(train_batch_len)\n        \n    train_loss = sum(train_loss) / sum(train_lens)\n    train_acc = sum(train_accs) / sum(train_lens)\n\n    # Print the information.\n    log(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n\n    # ---------- Validation ----------\n    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n    student_model.eval()\n\n    # These are used to record information in validation.\n    valid_loss = []\n    valid_accs = []\n    valid_lens = []\n\n    # Iterate the validation set by batches.\n    for batch in tqdm(valid_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        # We don't need gradient in validation.\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = student_model(imgs)\n            teacher_logits = teacher_model(imgs) # MEDIUM BASELINE\n\n        # We can still compute the loss (but not the gradient).\n        loss = loss_fn(logits, labels, teacher_logits) # MEDIUM BASELINE\n#         loss = loss_fn(logits, labels) # SIMPLE BASELINE\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels).float().sum()\n\n        # Record the loss and accuracy.\n        batch_len = len(imgs)\n        valid_loss.append(loss.item() * batch_len)\n        valid_accs.append(acc)\n        valid_lens.append(batch_len)\n        #break\n\n    # The average loss and accuracy for entire validation set is the average of the recorded values.\n    valid_loss = sum(valid_loss) / sum(valid_lens)\n    valid_acc = sum(valid_accs) / sum(valid_lens)\n\n    # update logs\n    \n    if valid_acc > best_acc:\n        log(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n    else:\n        log(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # save models\n    if valid_acc > best_acc:\n        log(f\"Best model found at epoch {epoch}, saving model\")\n        torch.save(student_model.state_dict(), f\"{save_path}/student_best.ckpt\") # only save best to prevent output memory exceed error\n        best_acc = valid_acc\n        stale = 0\n    else:\n        stale += 1\n        if stale > patience:\n            log(f\"No improvment {patience} consecutive epochs, early stopping\")\n            break\nlog(\"Finish training\")\nlog_fw.close()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T07:12:13.166912Z","iopub.execute_input":"2022-06-08T07:12:13.167661Z","iopub.status.idle":"2022-06-08T13:27:46.85227Z","shell.execute_reply.started":"2022-06-08T07:12:13.167627Z","shell.execute_reply":"2022-06-08T13:27:46.848837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"# create dataloader for evaluation\neval_set = FoodDataset(os.path.join(cfg['dataset_root'], \"evaluation\"), tfm=test_tfm)\neval_loader = DataLoader(eval_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:27:51.08796Z","iopub.execute_input":"2022-06-08T13:27:51.088804Z","iopub.status.idle":"2022-06-08T13:27:51.105481Z","shell.execute_reply.started":"2022-06-08T13:27:51.088767Z","shell.execute_reply":"2022-06-08T13:27:51.104357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model from {exp_name}/student_best.ckpt\nstudent_model_best = get_student_model() # get a new student model to avoid reference before assignment.\nckpt_path = f\"{save_path}/student_best.ckpt\" # the ckpt path of the best student model.\nstudent_model_best.load_state_dict(torch.load(ckpt_path, map_location='cpu')) # load the state dict and set it to the student model\nstudent_model_best.to(device) # set the student model to device\n\n# Start evaluate\nstudent_model_best.eval()\neval_preds = [] # storing predictions of the evaluation dataset\n\n# Iterate the validation set by batches.\nfor batch in tqdm(eval_loader):\n    # A batch consists of image data and corresponding labels.\n    imgs, _ = batch\n    # We don't need gradient in evaluation.\n    # Using torch.no_grad() accelerates the forward process.\n    with torch.no_grad():\n        logits = student_model_best(imgs.to(device))\n        preds = list(logits.argmax(dim=-1).squeeze().cpu().numpy())\n    # loss and acc can not be calculated because we do not have the true labels of the evaluation set.\n    eval_preds += preds\n\ndef pad4(i):\n    return \"0\"*(4-len(str(i))) + str(i)\n\n# Save prediction results\nids = [pad4(i) for i in range(0,len(eval_set))]\ncategories = eval_preds\n\ndf = pd.DataFrame()\ndf['Id'] = ids\ndf['Category'] = categories\ndf.to_csv(f\"./submission.csv\", index=False) # now you can download the submission.csv and upload it to the kaggle competition.","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:27:55.131153Z","iopub.execute_input":"2022-06-08T13:27:55.131799Z","iopub.status.idle":"2022-06-08T13:28:42.76392Z","shell.execute_reply.started":"2022-06-08T13:27:55.131762Z","shell.execute_reply":"2022-06-08T13:28:42.763169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}