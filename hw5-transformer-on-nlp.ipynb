{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### HW5 Transformer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# !nvidia-smi  # P100\n# RNN baseline(BLEU): 17.87\n# Transformer small(BLEU): 24.82\n# Transformer base: 25.82","metadata":{"execution":{"iopub.status.busy":"2022-05-14T03:08:51.497577Z","iopub.execute_input":"2022-05-14T03:08:51.498604Z","iopub.status.idle":"2022-05-14T03:08:51.52107Z","shell.execute_reply.started":"2022-05-14T03:08:51.498491Z","shell.execute_reply":"2022-05-14T03:08:51.520456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n!pip install --upgrade jupyter ipywidgets","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:03:32.337037Z","iopub.execute_input":"2022-05-15T02:03:32.337383Z","iopub.status.idle":"2022-05-15T02:03:53.768309Z","shell.execute_reply.started":"2022-05-15T02:03:32.337295Z","shell.execute_reply":"2022-05-15T02:03:53.767466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/pytorch/fairseq.git\n!cd fairseq && git checkout 9a1c497\n!pip install --upgrade ./fairseq/","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:03:53.771883Z","iopub.execute_input":"2022-05-15T02:03:53.772106Z","iopub.status.idle":"2022-05-15T02:04:58.830009Z","shell.execute_reply.started":"2022-05-15T02:03:53.772079Z","shell.execute_reply":"2022-05-15T02:04:58.829084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport pdb\nimport pprint\nimport logging\nimport os\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import data\nimport numpy as np\nimport tqdm.auto as tqdm\nfrom pathlib import Path\nfrom argparse import Namespace\nfrom fairseq import utils\n\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:04:58.833383Z","iopub.execute_input":"2022-05-15T02:04:58.833935Z","iopub.status.idle":"2022-05-15T02:05:02.258348Z","shell.execute_reply.started":"2022-05-15T02:04:58.833899Z","shell.execute_reply":"2022-05-15T02:05:02.257481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 设定种子","metadata":{}},{"cell_type":"code","source":"seed = 73\nrandom.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  \nnp.random.seed(seed)  \ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:05:02.260493Z","iopub.execute_input":"2022-05-15T02:05:02.260767Z","iopub.status.idle":"2022-05-15T02:05:02.27067Z","shell.execute_reply.started":"2022-05-15T02:05:02.260719Z","shell.execute_reply":"2022-05-15T02:05:02.269931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 资料集\n英语->繁体: 训练资料393,980句, 测试资料4000句","metadata":{}},{"cell_type":"markdown","source":"# 处理数据集","metadata":{}},{"cell_type":"markdown","source":"### 下载数据(raw.en -> train_dev.raw.en)","metadata":{}},{"cell_type":"code","source":"data_dir = './DATA/rawdata'\ndataset_name = 'ted2020'\nurls = (\n    # '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214989&authkey=AGgQ-DaR8eFSl1A\"', \n    # '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214987&authkey=AA4qP_azsicwZZM\"',\n# # If the above links die, use the following instead. \n    \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted2020.tgz\",\n    \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/test.tgz\",\n# # If the above links die, use the following instead. \n#     \"https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U\",\n#     \"https://mega.nz/#!zNcnGIoJ!oPJX9AvVVs11jc0SaK6vxP_lFUNTkEcK2WbxJpvjU5Y\",\n)\nfile_names = (\n    'ted2020.tgz', # train & dev\n    'test.tgz', # test\n)\nprefix = Path(data_dir).absolute() / dataset_name\nprefix.mkdir(parents=True, exist_ok=True)\nfor u, f in zip(urls, file_names):\n    path = prefix/f\n    if not path.exists():\n        if 'mega' in u:\n            !megadl {u} --path {path}\n        else:\n            !wget {u} -O {path}\n    if path.suffix == \".tgz\":\n        !tar -xvf {path} -C {prefix}\n    elif path.suffix == \".zip\":\n        !unzip -o {path} -d {prefix}\n!mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n!mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n!mv {prefix/'test.en'} {prefix/'test.raw.en'}\n!mv {prefix/'test.zh'} {prefix/'test.raw.zh'}","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:05:36.371367Z","iopub.execute_input":"2022-05-15T02:05:36.372021Z","iopub.status.idle":"2022-05-15T02:05:50.659523Z","shell.execute_reply.started":"2022-05-15T02:05:36.371979Z","shell.execute_reply":"2022-05-15T02:05:50.658539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### train_dev.raw.en -> train_dev.raw.clean.en","metadata":{}},{"cell_type":"code","source":"src_lang = 'en'\ntgt_lang = 'zh'\n\n# Back Translation\n# src_lang = 'zh'\n# tgt_lang = 'en'\n\ndata_prefix = f'{prefix}/train_dev.raw'\ntest_prefix = f'{prefix}/test.raw'","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:05:50.661644Z","iopub.execute_input":"2022-05-15T02:05:50.661882Z","iopub.status.idle":"2022-05-15T02:05:50.666297Z","shell.execute_reply.started":"2022-05-15T02:05:50.661854Z","shell.execute_reply":"2022-05-15T02:05:50.66557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head {data_prefix+'.'+src_lang} -n 5\n!head {data_prefix+'.'+tgt_lang} -n 5","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:05:50.667845Z","iopub.execute_input":"2022-05-15T02:05:50.668348Z","iopub.status.idle":"2022-05-15T02:05:52.051019Z","shell.execute_reply.started":"2022-05-15T02:05:50.668309Z","shell.execute_reply":"2022-05-15T02:05:52.049778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef strQ2B(ustring):\n    \"\"\"把字串全形轉半形\"\"\"\n    # 參考來源:https://ithelp.ithome.com.tw/articles/10233122\n    ss = []\n    for s in ustring:\n        rstring = \"\"\n        for uchar in s:\n            inside_code = ord(uchar)\n            if inside_code == 12288:  # 全形空格直接轉換\n                inside_code = 32\n            elif (inside_code >= 65281 and inside_code <= 65374):  # 全形字元（除空格）根據關係轉化，全角符号变半角\n                inside_code -= 65248\n            rstring += chr(inside_code)\n        ss.append(rstring)\n    return ''.join(ss)\n                \ndef clean_s(s, lang):\n    if lang == 'en':\n        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n        s = s.replace('-', '') # remove '-'\n        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation，\\1表示保留原字符,符号左右加空格，这样可以把符号也切出来(split)\n    elif lang == 'zh':\n        s = strQ2B(s) # Q2B\n        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n        s = s.replace(' ', '')\n        s = s.replace('—', '')\n        s = s.replace('“', '\"')\n        s = s.replace('”', '\"')\n        s = s.replace('_', '')\n        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation，符号左右加空格\n    s = ' '.join(s.strip().split())\n    return s\n\ndef len_s(s, lang):\n    if lang == 'zh':\n        return len(s)\n    return len(s.split())\n\ndef clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):  # 删一些异常的data pair\n    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n        return\n    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n                    for s1 in l1_in_f:\n                        s1 = s1.strip()\n                        s2 = l2_in_f.readline().strip()\n                        s1 = clean_s(s1, l1)\n                        s2 = clean_s(s2, l2)\n                        s1_len = len_s(s1, l1)\n                        s2_len = len_s(s2, l2)\n                        if min_len > 0: # remove short sentence\n                            if s1_len < min_len or s2_len < min_len:\n                                continue\n                        if max_len > 0: # remove long sentence\n                            if s1_len > max_len or s2_len > max_len:\n                                continue\n                        if ratio > 0: # remove by ratio of length\n                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n                                continue\n                        print(s1, file=l1_out_f)\n                        print(s2, file=l2_out_f)\nclean_corpus(data_prefix, src_lang, tgt_lang)  # data_prefix = train_dev.raw\nclean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)  # testdata 不清洗\n\n!head {data_prefix+'.clean.'+src_lang} -n 5\n!head {data_prefix+'.clean.'+tgt_lang} -n 5","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:05:52.054256Z","iopub.execute_input":"2022-05-15T02:05:52.054971Z","iopub.status.idle":"2022-05-15T02:06:12.545916Z","shell.execute_reply.started":"2022-05-15T02:05:52.054913Z","shell.execute_reply":"2022-05-15T02:06:12.544932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 生成train set、valid set(train_dev.raw.clean.en -> train.clean.en & valid.clean.en)","metadata":{}},{"cell_type":"code","source":"valid_ratio = 0.01 # 3000~4000句就夠了\ntrain_ratio = 1 - valid_ratio\n\nif (prefix/f'train.clean.{src_lang}').exists() \\\nand (prefix/f'train.clean.{tgt_lang}').exists() \\\nand (prefix/f'valid.clean.{src_lang}').exists() \\\nand (prefix/f'valid.clean.{tgt_lang}').exists():\n    print(f'train/valid splits exists. skipping split.')\nelse:\n    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n    labels = list(range(line_num))\n    random.shuffle(labels)\n    for lang in [src_lang, tgt_lang]:\n        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n        count = 0\n        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n            if labels[count]/line_num < train_ratio:\n                train_f.write(line)\n            else:\n                valid_f.write(line)\n            count += 1\n        train_f.close()\n        valid_f.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:06:12.548015Z","iopub.execute_input":"2022-05-15T02:06:12.548484Z","iopub.status.idle":"2022-05-15T02:06:13.913169Z","shell.execute_reply.started":"2022-05-15T02:06:12.548442Z","shell.execute_reply":"2022-05-15T02:06:13.912264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Subword Units \n翻譯存在的一大問題是未登錄詞(out of vocabulary)，可以使用 subword units 作為斷詞單位來解決。\n- 使用 [sentencepiece](#kudo-richardson-2018-sentencepiece) 套件\n- 用 unigram 或 byte-pair encoding (BPE)","metadata":{}},{"cell_type":"markdown","source":"### 训练分词器(生成spm8000.model&spm8000.vocab)","metadata":{}},{"cell_type":"code","source":"import sentencepiece as spm\nvocab_size = 8000\nif (prefix/f'spm{vocab_size}.model').exists():\n    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\nelse:\n    spm.SentencePieceTrainer.train(\n        input=','.join([f'{prefix}/train.clean.{src_lang}',\n                        f'{prefix}/valid.clean.{src_lang}',\n                        f'{prefix}/train.clean.{tgt_lang}',\n                        f'{prefix}/valid.clean.{tgt_lang}']),\n        model_prefix=prefix/f'spm{vocab_size}',\n        vocab_size=vocab_size,\n        character_coverage=1,\n        model_type='unigram', # 'bpe' 也可\n        input_sentence_size=1e6,\n        shuffle_input_sentence=True,\n        normalization_rule_name='nmt_nfkc_cf',\n    )","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:06:13.915286Z","iopub.execute_input":"2022-05-15T02:06:13.915542Z","iopub.status.idle":"2022-05-15T02:12:27.072269Z","shell.execute_reply.started":"2022-05-15T02:06:13.915493Z","shell.execute_reply":"2022-05-15T02:12:27.071197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 分词train.clean.en -> train.en\n下划线加词跟单独的词是两个不同的token","metadata":{}},{"cell_type":"code","source":"spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\nin_tag = {\n    'train': 'train.clean',\n    'valid': 'valid.clean',\n    'test': 'test.raw.clean',\n}\nfor split in ['train', 'valid', 'test']:\n    for lang in [src_lang, tgt_lang]:\n        out_path = prefix/f'{split}.{lang}'\n        if out_path.exists():\n            print(f\"{out_path} exists. skipping spm_encode.\")\n        else:\n            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n                    for line in in_f:\n                        line = line.strip()\n                        tok = spm_model.encode(line, out_type=str)\n                        print(' '.join(tok), file=out_f)\n\n!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:12:27.075336Z","iopub.execute_input":"2022-05-15T02:12:27.075672Z","iopub.status.idle":"2022-05-15T02:12:55.41392Z","shell.execute_reply.started":"2022-05-15T02:12:27.075639Z","shell.execute_reply":"2022-05-15T02:12:55.413023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install setuptools==59.5.0  # 版本不一致,下一个block会报错","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:12:55.415698Z","iopub.execute_input":"2022-05-15T02:12:55.415984Z","iopub.status.idle":"2022-05-15T02:13:14.739154Z","shell.execute_reply.started":"2022-05-15T02:12:55.415955Z","shell.execute_reply":"2022-05-15T02:13:14.73822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 用fairseq生成二进制文件(用于训练时读取数据)放到data-bin","metadata":{}},{"cell_type":"code","source":"binpath = Path('./DATA/data-bin', dataset_name)\nif binpath.exists():\n    print(binpath, \"exists, will not overwrite!\")\nelse:\n    !python -m fairseq_cli.preprocess \\\n        --source-lang {src_lang}\\\n        --target-lang {tgt_lang}\\\n        --trainpref {prefix/'train'}\\\n        --validpref {prefix/'valid'}\\\n        --testpref {prefix/'test'}\\\n        --destdir {binpath}\\\n        --joined-dictionary\\\n        --workers 2","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:13:14.742537Z","iopub.execute_input":"2022-05-15T02:13:14.742766Z","iopub.status.idle":"2022-05-15T02:16:46.21419Z","shell.execute_reply.started":"2022-05-15T02:13:14.742738Z","shell.execute_reply":"2022-05-15T02:16:46.213139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练参数设置\nNamespace就像一个类一样,可以访问类属性</br>\nEx:</br>\na = Namespace(dd = 'dyx')</br>\nprint(a.dd)</br>\n打印dyx","metadata":{}},{"cell_type":"code","source":"config = Namespace(\n    datadir = \"./DATA/data-bin/ted2020\",\n#     savedir = \"./checkpoints/rnn\",\n    savedir = \"./checkpoints/transformer_base\",\n    # Back Translation\n    source_lang = \"en\",\n    target_lang = \"zh\",\n    # Back Translation\n#     source_lang = \"zh\",\n#     target_lang = \"en\",\n    \n    # cpu threads when fetching & processing data.\n    num_workers=2,  \n    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n    max_tokens=8192,\n    accum_steps=2,  # 累积两次batch的梯度更新,相当于把batch size提高了两倍\n    \n    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n    lr_factor=2.,\n    lr_warmup=4000,\n    \n    # clipping gradient norm helps alleviate gradient exploding\n    clip_norm=1.0,\n    \n    # maximum epochs for training\n    max_epoch=30,\n    start_epoch=1,\n    \n    # beam size for beam search\n    beam=5, \n    # generate sequences of maximum length ax + b, where x is the source length\n    # 不让他无限产生句子来水准确率\n    max_len_a=1.2, \n    max_len_b=10,\n    # when decoding, post process sentence by removing sentencepiece symbols.删掉字之前的下划线\n    post_process = \"sentencepiece\",\n    \n    # checkpoints,输出最后5个epoch的预测值\n    keep_last_epochs=5,\n    resume=None, # if resume from checkpoint name (under config.savedir)\n    \n    # logging\n    use_wandb=False,\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:46.216257Z","iopub.execute_input":"2022-05-15T02:16:46.216566Z","iopub.status.idle":"2022-05-15T02:16:46.226313Z","shell.execute_reply.started":"2022-05-15T02:16:46.216527Z","shell.execute_reply":"2022-05-15T02:16:46.225425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 日志","metadata":{}},{"cell_type":"code","source":"logging.basicConfig(\n    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n    stream=sys.stdout,\n)\nproj = \"hw5.seq2seq\"\nlogger = logging.getLogger(proj)\nch = logging.StreamHandler()\nlogger.addHandler(ch)\nif config.use_wandb:\n    import wandb\n    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:46.227722Z","iopub.execute_input":"2022-05-15T02:16:46.228386Z","iopub.status.idle":"2022-05-15T02:16:46.238566Z","shell.execute_reply.started":"2022-05-15T02:16:46.228348Z","shell.execute_reply":"2022-05-15T02:16:46.237571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 读取数据集(使用fairseq里的translation task)","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nfrom fairseq.tasks.translation import TranslationConfig, TranslationTask\n\n## setup task\ntask_cfg = TranslationConfig(\n    data=config.datadir,\n    source_lang=config.source_lang,\n    target_lang=config.target_lang,\n    train_subset=\"train\",\n    required_seq_len_multiple=8,\n    dataset_impl=\"mmap\",\n    upsample_primary=1,\n)\ntask = TranslationTask.setup_task(task_cfg)\n\nlogger.info(\"loading data for epoch 1\")\ntask.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\ntask.load_dataset(split=\"valid\", epoch=1)\n\n# 打印样本\nsample = task.dataset(\"valid\")[1]\npprint.pprint(sample)\npprint.pprint(\n    \"Source: \" + \\\n    task.source_dictionary.string(\n        sample['source'],\n        config.post_process,\n    )\n)\npprint.pprint(\n    \"Target: \" + \\\n    task.target_dictionary.string(\n        sample['target'],\n        config.post_process,\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:46.240255Z","iopub.execute_input":"2022-05-15T02:16:46.240829Z","iopub.status.idle":"2022-05-15T02:16:46.351198Z","shell.execute_reply.started":"2022-05-15T02:16:46.240788Z","shell.execute_reply":"2022-05-15T02:16:46.350501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n    batch_iterator = task.get_batch_iterator(\n        dataset=task.dataset(split),\n        max_tokens=max_tokens,\n        max_sentences=None,\n        max_positions=utils.resolve_max_positions(\n            task.max_positions(),\n            max_tokens,\n        ),\n        ignore_invalid_inputs=True,\n        seed=seed,\n        num_workers=num_workers,\n        epoch=epoch,\n        disable_iterator_cache=not cached,\n        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n        # first call of this method has no effect. \n    )\n    return batch_iterator\n\ndemo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\ndemo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\nsample = next(demo_iter)\nsample","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:46.352485Z","iopub.execute_input":"2022-05-15T02:16:46.352832Z","iopub.status.idle":"2022-05-15T02:16:46.441904Z","shell.execute_reply.started":"2022-05-15T02:16:46.352791Z","shell.execute_reply":"2022-05-15T02:16:46.440976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 每個 batch 是一個字典，key 是字串，value 是 Tensor，內容說明如下\n```python\nbatch = {\n    \"id\": id, # 每個 example 的 id\n    \"nsentences\": len(samples), # batch size 句子數\n    \"ntokens\": ntokens, # batch size 字數\n    \"net_input\": {\n        \"src_tokens\": src_tokens, # 來源語言的序列\n        \"src_lengths\": src_lengths, # 每句話沒有 pad 過的長度\n        \"prev_output_tokens\": prev_output_tokens, # 上面提到右 shift 一格後的目標序列\n    },\n    \"target\": target, # 目標序列\n}\n```","metadata":{}},{"cell_type":"markdown","source":"# 模型架构(Transformer)","metadata":{}},{"cell_type":"code","source":"from fairseq.models import (\n    FairseqEncoder, \n    FairseqIncrementalDecoder,\n    FairseqEncoderDecoderModel\n)\nclass Seq2Seq(FairseqEncoderDecoderModel):\n    def __init__(self, args, encoder, decoder):\n        super().__init__(encoder, decoder)\n        self.args = args\n    \n    def forward(\n        self,\n        src_tokens,\n        src_lengths,\n        prev_output_tokens,\n        return_all_hiddens: bool = True,\n    ):\n        \"\"\"\n        Run the forward pass for an encoder-decoder model.\n        \"\"\"\n        encoder_out = self.encoder(\n            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n        )\n        logits, extra = self.decoder(\n            prev_output_tokens,\n            encoder_out=encoder_out,\n            src_lengths=src_lengths,\n            return_all_hiddens=return_all_hiddens,\n        )\n        return logits, extra\n\n    \n# HINT: transformer 架構\nfrom fairseq.models.transformer import (\n    TransformerEncoder, \n    TransformerDecoder,\n)\n\ndef build_model(args, task):\n    \"\"\" 按照參數設定建置模型 \"\"\"\n    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n\n    # 詞嵌入\n    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n    \n    # 編碼器與解碼器\n    # TODO: 替換成 TransformerEncoder 和 TransformerDecoder\n    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n    \n    # 序列到序列模型\n    model = Seq2Seq(args, encoder, decoder)\n    \n    # 序列到序列模型的初始化很重要 需要特別處理\n    def init_params(module):\n        from fairseq.modules import MultiheadAttention\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        if isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        if isinstance(module, MultiheadAttention):\n            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n        if isinstance(module, nn.RNNBase):\n            for name, param in module.named_parameters():\n                if \"weight\" in name or \"bias\" in name:\n                    param.data.uniform_(-0.1, 0.1)\n            \n    # 初始化模型\n    model.apply(init_params)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:46.444403Z","iopub.execute_input":"2022-05-15T02:16:46.445169Z","iopub.status.idle":"2022-05-15T02:16:46.463444Z","shell.execute_reply.started":"2022-05-15T02:16:46.445126Z","shell.execute_reply":"2022-05-15T02:16:46.462583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 模型参数","metadata":{}},{"cell_type":"code","source":"# small transformer参数\n# arch_args = Namespace(\n#     encoder_embed_dim=256,\n#     encoder_ffn_embed_dim=1024,\n#     encoder_layers=4,\n#     decoder_embed_dim=256,\n#     decoder_ffn_embed_dim=1024,\n#     decoder_layers=4,\n#     share_decoder_input_output_embed=True,\n#     dropout=0.1,\n# )\n\n# # HINT: 補上Transformer用的參數\n# def add_transformer_args(args):\n#     args.encoder_attention_heads=4\n#     args.encoder_normalize_before=True\n    \n#     args.decoder_attention_heads=4\n#     args.decoder_normalize_before=True\n    \n#     args.activation_fn=\"relu\"\n#     args.max_source_positions=1024\n#     args.max_target_positions=1024\n    \n#     # 補上我們沒有設定的Transformer預設參數\n#     from fairseq.models.transformer import base_architecture \n#     base_architecture(arch_args)\n\narch_args = Namespace(\n    encoder_embed_dim=512,\n    encoder_ffn_embed_dim=2048,\n    encoder_layers=6,\n    decoder_embed_dim=512,\n    decoder_ffn_embed_dim=2048,\n    decoder_layers=6,\n    share_decoder_input_output_embed=True,\n    dropout=0.1,\n)\n\n# HINT: 補上Transformer用的參數\ndef add_transformer_args(args):\n    args.encoder_attention_heads=8\n    args.encoder_normalize_before=True\n    \n    args.decoder_attention_heads=8\n    args.decoder_normalize_before=True\n    \n    args.activation_fn=\"relu\"\n    args.max_source_positions=1024\n    args.max_target_positions=1024\n    \n    # 補上我們沒有設定的Transformer預設參數\n    from fairseq.models.transformer import base_architecture \n    base_architecture(arch_args)\n  \nadd_transformer_args(arch_args)\nmodel = build_model(arch_args, task)\nlogger.info(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:46.46785Z","iopub.execute_input":"2022-05-15T02:16:46.468465Z","iopub.status.idle":"2022-05-15T02:16:47.772325Z","shell.execute_reply.started":"2022-05-15T02:16:46.468416Z","shell.execute_reply":"2022-05-15T02:16:47.771586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 损失函数使用 Label Smoothing Regularization过的CE\n### (可以让模型输出概率不过于集中,防止模型过度自信)","metadata":{}},{"cell_type":"code","source":"class LabelSmoothedCrossEntropyCriterion(nn.Module):\n    def __init__(self, smoothing, ignore_index=None, reduce=True):\n        super().__init__()\n        self.smoothing = smoothing\n        self.ignore_index = ignore_index\n        self.reduce = reduce\n    \n    def forward(self, lprobs, target):\n        if target.dim() == lprobs.dim() - 1:\n            target = target.unsqueeze(-1)\n        # nll: Negative log likelihood，當目標是one-hot時的cross-entropy loss. 以下同 F.nll_loss\n        nll_loss = -lprobs.gather(dim=-1, index=target)\n        # 將一部分正確答案的機率分配給其他label 所以當計算cross-entropy時等於把所有label的log prob加起來\n        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n        if self.ignore_index is not None:\n            pad_mask = target.eq(self.ignore_index)\n            nll_loss.masked_fill_(pad_mask, 0.0)\n            smooth_loss.masked_fill_(pad_mask, 0.0)\n        else:\n            nll_loss = nll_loss.squeeze(-1)\n            smooth_loss = smooth_loss.squeeze(-1)\n        if self.reduce:\n            nll_loss = nll_loss.sum()\n            smooth_loss = smooth_loss.sum()\n        # 計算cross-entropy時 加入分配給其他label的loss\n        eps_i = self.smoothing / lprobs.size(-1)\n        # 左边是label对应的log prob,右边是其余所有log prob的和,这样可以避免其他选项的概率趋于0\n        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n        return loss\n\n# 一般都用0.1效果就很好了\ncriterion = LabelSmoothedCrossEntropyCriterion(\n    smoothing=0.1,\n    ignore_index=task.target_dictionary.pad(),\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:47.77387Z","iopub.execute_input":"2022-05-15T02:16:47.77438Z","iopub.status.idle":"2022-05-15T02:16:47.784938Z","shell.execute_reply.started":"2022-05-15T02:16:47.774337Z","shell.execute_reply":"2022-05-15T02:16:47.78426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimizer: Adam + lr scheduling\n前期线性增长，后期根据步数的根号倒数衰减\n$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$","metadata":{}},{"cell_type":"code","source":"class NoamOpt:\n    \"Optim wrapper that implements rate.\"\n    def __init__(self, model_size, factor, warmup, optimizer):\n        self.optimizer = optimizer\n        self._step = 0\n        self.warmup = warmup\n        self.factor = factor\n        self.model_size = model_size\n        self._rate = 0\n    \n    @property\n    def param_groups(self):\n        return self.optimizer.param_groups\n        \n    def multiply_grads(self, c):\n        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is not None:\n                    p.grad.data.mul_(c)\n        \n    def step(self):\n        \"Update parameters and rate\"\n        self._step += 1\n        rate = self.rate()\n        for p in self.param_groups:\n            p['lr'] = rate\n        self._rate = rate\n        self.optimizer.step()\n        \n    def rate(self, step = None):\n        \"Implement `lrate` above\"\n        if step is None:\n            step = self._step\n        return 0 if not step else self.factor * \\\n            (self.model_size ** (-0.5) *\n            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n    \n# 可视化lr\noptimizer = NoamOpt(\n    model_size=arch_args.encoder_embed_dim, \n    factor=config.lr_factor, \n    warmup=config.lr_warmup, \n    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\nplt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\nplt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\nNone","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:47.786361Z","iopub.execute_input":"2022-05-15T02:16:47.786859Z","iopub.status.idle":"2022-05-15T02:16:48.256135Z","shell.execute_reply.started":"2022-05-15T02:16:47.786817Z","shell.execute_reply":"2022-05-15T02:16:48.255387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 训练函数","metadata":{}},{"cell_type":"code","source":"from fairseq.data import iterators\nfrom torch.cuda.amp import GradScaler, autocast\n\ndef train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n    itr = epoch_itr.next_epoch_itr(shuffle=True)\n    itr = iterators.GroupedIterator(itr, accum_steps) # 梯度累積: 每 accum_steps 個 sample 更新一次\n    \n    stats = {\"loss\": []}\n    scaler = GradScaler() # 混和精度訓練 automatic mixed precision (amp) \n    \n    model.train()\n    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n    for samples in progress:\n        model.zero_grad()\n        accum_loss = 0\n        sample_size = 0\n        # 梯度累積: 每 accum_steps 個 sample 更新一次\n        for i, sample in enumerate(samples):\n            if i == 1:\n                # emptying the CUDA cache after the first step can reduce the chance of OOM\n                torch.cuda.empty_cache()\n\n            sample = utils.move_to_cuda(sample, device=device)\n            target = sample[\"target\"]\n            sample_size_i = sample[\"ntokens\"]\n            sample_size += sample_size_i\n            \n            # 混和精度訓練 \n            with autocast():\n                net_output = model.forward(**sample[\"net_input\"])\n                lprobs = F.log_softmax(net_output[0], -1)            \n                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n                \n                # logging\n                accum_loss += loss.item()\n                # back-prop\n                scaler.scale(loss).backward()                \n        \n        scaler.unscale_(optimizer)\n        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # 梯度裁剪 防止梯度爆炸\n        \n        scaler.step(optimizer)\n        scaler.update()\n        \n        # logging\n        loss_print = accum_loss/sample_size\n        stats[\"loss\"].append(loss_print)\n        progress.set_postfix(loss=loss_print)\n        if config.use_wandb:\n            wandb.log({\n                \"train/loss\": loss_print,\n                \"train/grad_norm\": gnorm.item(),\n                \"train/lr\": optimizer.rate(),\n                \"train/sample_size\": sample_size,\n            })\n        \n    loss_print = np.mean(stats[\"loss\"])\n    logger.info(f\"training loss: {loss_print:.4f}\")\n    return stats","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:48.257925Z","iopub.execute_input":"2022-05-15T02:16:48.258417Z","iopub.status.idle":"2022-05-15T02:16:48.273597Z","shell.execute_reply.started":"2022-05-15T02:16:48.258374Z","shell.execute_reply":"2022-05-15T02:16:48.272573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation(带beam search)","metadata":{}},{"cell_type":"code","source":"# fairseq 的 beam search generator\n# 給定模型和輸入序列，用 beam search 生成翻譯結果\nsequence_generator = task.build_generator([model], config)\n\ndef decode(toks, dictionary):\n    # 從 Tensor 轉成人看得懂的句子\n    s = dictionary.string(\n        toks.int().cpu(),\n        config.post_process,\n    )\n    return s if s else \"<unk>\"\n\ndef inference_step(sample, model):\n    gen_out = sequence_generator.generate([model], sample)\n    srcs = []\n    hyps = []\n    refs = []\n    for i in range(len(gen_out)):\n        # 對於每個 sample, 收集輸入，輸出和參考答案，稍後計算 BLEU\n        srcs.append(decode(\n            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n            task.source_dictionary,\n        ))\n        hyps.append(decode(\n            gen_out[i][0][\"tokens\"], # 0 代表取出 beam 內分數第一的輸出結果\n            task.target_dictionary,\n        ))\n        refs.append(decode(\n            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n            task.target_dictionary,\n        ))\n    return srcs, hyps, refs\n\nimport shutil\nimport sacrebleu\n\ndef validate(model, task, criterion, log_to_wandb=True):\n    logger.info('begin validation')\n    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n    \n    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n    srcs = []\n    hyps = []\n    refs = []\n    \n    model.eval()\n    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n    with torch.no_grad():\n        for i, sample in enumerate(progress):\n            # validation loss\n            sample = utils.move_to_cuda(sample, device=device)\n            net_output = model.forward(**sample[\"net_input\"])\n\n            lprobs = F.log_softmax(net_output[0], -1)\n            target = sample[\"target\"]\n            sample_size = sample[\"ntokens\"]\n            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n            progress.set_postfix(valid_loss=loss.item())\n            stats[\"loss\"].append(loss)\n            \n            # 進行推論\n            s, h, r = inference_step(sample, model)\n            srcs.extend(s)\n            hyps.extend(h)\n            refs.extend(r)\n            \n    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n    stats[\"srcs\"] = srcs\n    stats[\"hyps\"] = hyps\n    stats[\"refs\"] = refs\n    \n    if config.use_wandb and log_to_wandb:\n        wandb.log({\n            \"valid/loss\": stats[\"loss\"],\n            \"valid/bleu\": stats[\"bleu\"].score,\n        }, commit=False)\n    \n    showid = np.random.randint(len(hyps))\n    logger.info(\"example source: \" + srcs[showid])\n    logger.info(\"example hypothesis: \" + hyps[showid])\n    logger.info(\"example reference: \" + refs[showid])\n    \n    # show bleu results\n    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n    logger.info(stats[\"bleu\"].format())\n    return stats","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:48.275445Z","iopub.execute_input":"2022-05-15T02:16:48.275781Z","iopub.status.idle":"2022-05-15T02:16:48.326404Z","shell.execute_reply.started":"2022-05-15T02:16:48.275741Z","shell.execute_reply":"2022-05-15T02:16:48.325651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 模型保存&加载","metadata":{}},{"cell_type":"code","source":"def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n    stats = validate(model, task, criterion)\n    bleu = stats['bleu']\n    loss = stats['loss']\n    if save:\n        # save epoch checkpoints\n        savedir = Path(config.savedir).absolute()\n        savedir.mkdir(parents=True, exist_ok=True)\n        \n        check = {\n            \"model\": model.state_dict(),\n            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n            \"optim\": {\"step\": optimizer._step}\n        }\n        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n    \n        # save epoch samples\n        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n                f.write(f\"{s}\\t{h}\\n\")\n\n        # get best valid bleu    \n        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n            validate_and_save.best_bleu = bleu.score\n            torch.save(check, savedir/f\"checkpoint_best.pt\")\n            \n        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n        if del_file.exists():\n            del_file.unlink()\n    \n    return stats\n\ndef try_load_checkpoint(model, optimizer=None, name=None):\n    name = name if name else \"checkpoint_last.pt\"\n    checkpath = Path(config.savedir)/name\n    if checkpath.exists():\n        check = torch.load(checkpath)\n        model.load_state_dict(check[\"model\"])\n        stats = check[\"stats\"]\n        step = \"unknown\"\n        if optimizer != None:\n            optimizer._step = step = check[\"optim\"][\"step\"]\n        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n    else:\n        logger.info(f\"no checkpoints found at {checkpath}!\")","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:48.327982Z","iopub.execute_input":"2022-05-15T02:16:48.328594Z","iopub.status.idle":"2022-05-15T02:16:48.342185Z","shell.execute_reply.started":"2022-05-15T02:16:48.328555Z","shell.execute_reply":"2022-05-15T02:16:48.341427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 主程序","metadata":{}},{"cell_type":"code","source":"model = model.to(device=device)\ncriterion = criterion.to(device=device)\n\nlogger.info(\"task: {}\".format(task.__class__.__name__))\nlogger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\nlogger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\nlogger.info(\"criterion: {}\".format(criterion.__class__.__name__))\nlogger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\nlogger.info(\n    \"num. model params: {:,} (num. trained: {:,})\".format(\n        sum(p.numel() for p in model.parameters()),\n        sum(p.numel() for p in model.parameters() if p.requires_grad),\n    )\n)\nlogger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:48.344297Z","iopub.execute_input":"2022-05-15T02:16:48.344893Z","iopub.status.idle":"2022-05-15T02:16:51.446686Z","shell.execute_reply.started":"2022-05-15T02:16:48.344851Z","shell.execute_reply":"2022-05-15T02:16:51.445969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n# try_load_checkpoint(model, optimizer, name=config.resume)\nwhile epoch_itr.next_epoch_idx <= config.max_epoch:\n    # train for one epoch\n    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T02:16:51.448213Z","iopub.execute_input":"2022-05-15T02:16:51.448819Z","iopub.status.idle":"2022-05-15T10:05:50.721652Z","shell.execute_reply.started":"2022-05-15T02:16:51.448761Z","shell.execute_reply":"2022-05-15T10:05:50.720826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget --help","metadata":{"execution":{"iopub.status.busy":"2022-05-15T10:23:38.544617Z","iopub.execute_input":"2022-05-15T10:23:38.544939Z","iopub.status.idle":"2022-05-15T10:23:39.344477Z","shell.execute_reply.started":"2022-05-15T10:23:38.544904Z","shell.execute_reply":"2022-05-15T10:23:39.343521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 生成sub文件","metadata":{}},{"cell_type":"markdown","source":"### 生成最后5个epoch合成的ensemble模型","metadata":{}},{"cell_type":"code","source":"# 把幾個 checkpoint 平均起來可以達到 ensemble 的效果\ncheckdir=config.savedir\n!python ./fairseq/scripts/average_checkpoints.py \\\n--inputs {checkdir} \\\n--num-epoch-checkpoints 5 \\\n--output {checkdir}/avg_last_5_checkpoint.pt","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:33:28.935443Z","iopub.execute_input":"2022-05-14T07:33:28.935879Z","iopub.status.idle":"2022-05-14T07:33:29.015381Z","shell.execute_reply.started":"2022-05-14T07:33:28.935791Z","shell.execute_reply":"2022-05-14T07:33:29.014223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 加载ensemble模型","metadata":{}},{"cell_type":"code","source":"# checkpoint_last.pt : 最後一次檢驗的檔案\n# checkpoint_best.pt : 檢驗 BLEU 最高的檔案\n# avg_last_5_checkpoint.pt:　最5後個檔案平均\ntry_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\nvalidate(model, task, criterion, log_to_wandb=False)\nNone","metadata":{"execution":{"iopub.status.busy":"2022-05-14T03:33:29.000921Z","iopub.status.idle":"2022-05-14T03:33:29.001545Z","shell.execute_reply.started":"2022-05-14T03:33:29.001312Z","shell.execute_reply":"2022-05-14T03:33:29.001336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 预测","metadata":{}},{"cell_type":"code","source":"def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):    \n    task.load_dataset(split=split, epoch=1)\n    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n    \n    idxs = []\n    hyps = []\n\n    model.eval()\n    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n    with torch.no_grad():\n        for i, sample in enumerate(progress):\n            # validation loss\n            sample = utils.move_to_cuda(sample, device=device)\n\n            # 進行推論\n            s, h, r = inference_step(sample, model)\n            \n            hyps.extend(h)\n            idxs.extend(list(sample['id']))\n            \n    # 根據 preprocess 時的順序排列\n    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n    \n    with open(outfile, \"w\") as f:\n        for h in hyps:\n            f.write(h+\"\\n\")\n\ngenerate_prediction(model, task)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T03:33:29.00275Z","iopub.status.idle":"2022-05-14T03:33:29.003384Z","shell.execute_reply.started":"2022-05-14T03:33:29.00315Z","shell.execute_reply":"2022-05-14T03:33:29.003175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 反向翻译部分","metadata":{}},{"cell_type":"code","source":"# data_dir = './DATA/rawdata'\n# mono_dataset_name = 'mono'\n# mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n# mono_prefix.mkdir(parents=True, exist_ok=True)\n\n# urls = (\n# #     '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214986&authkey=AANUKbGfZx0kM80\"',\n# # # If the above links die, use the following instead. \n#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted_zh_corpus.deduped.gz\",\n# # # If the above links die, use the following instead. \n# #     \"https://mega.nz/#!vMNnDShR!4eHDxzlpzIpdpeQTD-htatU_C7QwcBTwGDaSeBqH534\",\n# )\n# file_names = (\n#     'ted_zh_corpus.deduped.gz',\n# )\n\n# for u, f in zip(urls, file_names):\n#     path = mono_prefix/f\n# #     if not path.exists():\n#     if 'mega' in u:\n#         !megadl {u} --path {path}\n#     else:\n#         !wget {u} -O {path}\n# #     else:\n# #         print(f'{f} is exist, skip downloading')\n#     if path.suffix == \".tgz\":\n#         !tar -xvf {path} -C {prefix}\n#     elif path.suffix == \".zip\":\n#         !unzip -o {path} -d {prefix}\n#     elif path.suffix == \".gz\":\n#         !gzip -fkd {path}","metadata":{"execution":{"iopub.status.busy":"2022-05-15T01:49:43.690172Z","iopub.execute_input":"2022-05-15T01:49:43.690781Z","iopub.status.idle":"2022-05-15T01:49:52.198406Z","shell.execute_reply.started":"2022-05-15T01:49:43.690742Z","shell.execute_reply":"2022-05-15T01:49:52.197412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 打包工作区所有文件\nimport os\nimport zipfile\nimport datetime\n\ndef file2zip(packagePath, zipPath):\n    '''\n  :param packagePath: 文件夹路径\n  :param zipPath: 压缩包路径\n  :return:\n  '''\n    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n    for path, dirNames, fileNames in os.walk(packagePath):\n        fpath = path.replace(packagePath, '')\n        for name in fileNames:\n            fullName = os.path.join(path, name)\n            name = fpath + '\\\\' + name\n            zip.write(fullName, name)\n    zip.close()\n\n\nif __name__ == \"__main__\":\n    # 文件夹路径\n    packagePath = '/kaggle/working/'\n    zipPath = '/kaggle/working/output.zip'\n    if os.path.exists(zipPath):\n        os.remove(zipPath)\n    file2zip(packagePath, zipPath)\n    print(\"打包完成\")\n    print(datetime.datetime.utcnow())","metadata":{},"execution_count":null,"outputs":[]}]}