{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GAN生成动漫头像","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# You may replace the workspace directory if you want.\nworkspace_dir = './'\n\n# Training progress bar\n!pip install -q qqdm","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:36:02.11011Z","iopub.execute_input":"2022-05-19T07:36:02.110488Z","iopub.status.idle":"2022-05-19T07:36:15.848033Z","shell.execute_reply.started":"2022-05-19T07:36:02.110404Z","shell.execute_reply":"2022-05-19T07:36:15.847064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:36:15.850494Z","iopub.execute_input":"2022-05-19T07:36:15.850814Z","iopub.status.idle":"2022-05-19T07:36:16.71306Z","shell.execute_reply.started":"2022-05-19T07:36:15.850766Z","shell.execute_reply":"2022-05-19T07:36:16.712208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nimport torch\nimport numpy as np\n\n\ndef same_seeds(seed):\n    # Python built-in random module\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    # Torch\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nsame_seeds(2021)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:36:16.714308Z","iopub.execute_input":"2022-05-19T07:36:16.714518Z","iopub.status.idle":"2022-05-19T07:36:18.536743Z","shell.execute_reply.started":"2022-05-19T07:36:16.714492Z","shell.execute_reply":"2022-05-19T07:36:18.53596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom qqdm.notebook import qqdm","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:36:18.539658Z","iopub.execute_input":"2022-05-19T07:36:18.540154Z","iopub.status.idle":"2022-05-19T07:36:18.772348Z","shell.execute_reply.started":"2022-05-19T07:36:18.540112Z","shell.execute_reply":"2022-05-19T07:36:18.771674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CrypkoDataset(Dataset):\n    def __init__(self, fnames, transform):\n        self.transform = transform\n        self.fnames = fnames\n        self.num_samples = len(self.fnames)\n\n    def __getitem__(self,idx):\n        fname = self.fnames[idx]\n        # 1. Load the image\n        img = torchvision.io.read_image(fname)\n        # 2. Resize and normalize the images using torchvision.\n        img = self.transform(img)\n        return img\n\n    def __len__(self):\n        return self.num_samples\n\n\ndef get_dataset(root):\n    fnames = glob.glob(os.path.join(root, '*'))\n    # 1. Resize the image to (64, 64)\n    # 2. Linearly map [0, 1] to [-1, 1]\n    compose = [\n        transforms.ToPILImage(),\n        transforms.Resize((64, 64)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),  # [0, 1] -> [-1, 1]\n    ]\n    transform = transforms.Compose(compose)\n    dataset = CrypkoDataset(fnames, transform)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:36:18.774718Z","iopub.execute_input":"2022-05-19T07:36:18.775202Z","iopub.status.idle":"2022-05-19T07:36:18.784892Z","shell.execute_reply.started":"2022-05-19T07:36:18.775163Z","shell.execute_reply":"2022-05-19T07:36:18.784093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = get_dataset(os.path.join('../input/hw6generative-adversarial-network', 'faces'))\nimages = [(dataset[i]+1)/2 for i in range(16)]\ngrid_img = torchvision.utils.make_grid(images, nrow=4)\nplt.figure(figsize=(10,10))\nplt.imshow(grid_img.permute(1, 2, 0))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:36:18.785849Z","iopub.execute_input":"2022-05-19T07:36:18.78605Z","iopub.status.idle":"2022-05-19T07:36:20.457396Z","shell.execute_reply.started":"2022-05-19T07:36:18.78602Z","shell.execute_reply":"2022-05-19T07:36:20.456651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\nclass Generator(nn.Module):\n    \"\"\"\n    Input shape: (N, in_dim)\n    Output shape: (N, 3, 64, 64)\n    \"\"\"\n    def __init__(self, in_dim, dim=64):\n        super(Generator, self).__init__()\n        def dconv_bn_relu(in_dim, out_dim):\n            return nn.Sequential(\n                nn.ConvTranspose2d(in_dim, out_dim, 5, 2, # kernel_size = 5, stride = 2\n                                   padding=2, output_padding=1, bias=False),\n                nn.BatchNorm2d(out_dim),\n                nn.ReLU()\n            )\n        self.l1 = nn.Sequential(\n            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n            nn.BatchNorm1d(dim * 8 * 4 * 4),\n            nn.ReLU()\n        )\n        self.l2_5 = nn.Sequential(\n            dconv_bn_relu(dim * 8, dim * 4),  # [dim*8, 4, 4] -> [dim*4, 8, 8]\n            dconv_bn_relu(dim * 4, dim * 2),  # [dim*4, 8, 8] -> [dim*2, 16, 16]\n            dconv_bn_relu(dim * 2, dim),     # [dim*2, 16, 16] -> [dim, 32, 32]\n            nn.ConvTranspose2d(dim, 3, 5, 2, padding=2, output_padding=1),  # [dim, 32, 32] -> [3, 64, 64]\n            nn.Tanh()\n        )\n        self.apply(weights_init)\n\n    def forward(self, x):\n        y = self.l1(x)              # [dim*8*4*4, 1, 1]\n        y = y.view(y.size(0), -1, 4, 4)    # [dim*8, 4, 4]\n        y = self.l2_5(y)             # [3, 64, 64]\n        return y\n\n\nclass Discriminator(nn.Module):\n    \"\"\"\n    Input shape: (N, 3, 64, 64)\n    Output shape: (N, )\n    \"\"\"\n    def __init__(self, in_dim, dim=64):\n        super(Discriminator, self).__init__()\n\n        def conv_bn_lrelu(in_dim, out_dim):\n            return nn.Sequential(\n                nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n                nn.BatchNorm2d(out_dim),\n                nn.LeakyReLU(0.2),\n            )\n            \n        \"\"\" Medium: Remove the last sigmoid layer for WGAN. \"\"\"\n        self.ls = nn.Sequential(\n            nn.Conv2d(in_dim, dim, 5, 2, 2), \n            nn.LeakyReLU(0.2),\n            conv_bn_lrelu(dim, dim * 2),\n            conv_bn_lrelu(dim * 2, dim * 4),\n            conv_bn_lrelu(dim * 4, dim * 8),\n            nn.Conv2d(dim * 8, 1, 4),\n            # nn.Sigmoid(), \n        )\n        self.apply(weights_init)\n        \n    def forward(self, x):\n        y = self.ls(x)\n        y = y.view(-1)\n        return y","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:36:20.458381Z","iopub.execute_input":"2022-05-19T07:36:20.458599Z","iopub.status.idle":"2022-05-19T07:36:20.481723Z","shell.execute_reply.started":"2022-05-19T07:36:20.458571Z","shell.execute_reply":"2022-05-19T07:36:20.480836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training hyperparameters\nbatch_size = 64\nz_dim = 100\nz_sample = Variable(torch.randn(100, z_dim)).cuda()\nlr = 1e-4\n\n\"\"\" Medium: WGAN, 50 epoch, n_critic=5, clip_value=0.01 \"\"\"\nn_epoch = 50 # 50\nn_critic = 5 # 5\nclip_value = 0.01\n\nlog_dir = os.path.join(workspace_dir, 'logs')\nckpt_dir = os.path.join(workspace_dir, 'checkpoints')\nos.makedirs(log_dir, exist_ok=True)\nos.makedirs(ckpt_dir, exist_ok=True)\n\n# Model\nG = Generator(in_dim=z_dim).cuda()\nD = Discriminator(3).cuda()\nG.train()\nD.train()\n\n# Loss\ncriterion = nn.BCELoss()\n\n\"\"\" Medium: Use RMSprop for WGAN. \"\"\"\n# Optimizer\n# opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n# opt_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\nopt_D = torch.optim.RMSprop(D.parameters(), lr=lr)\nopt_G = torch.optim.RMSprop(G.parameters(), lr=lr)\n\n\n# DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:36:20.483615Z","iopub.execute_input":"2022-05-19T07:36:20.484187Z","iopub.status.idle":"2022-05-19T07:36:23.391544Z","shell.execute_reply.started":"2022-05-19T07:36:20.484146Z","shell.execute_reply":"2022-05-19T07:36:23.39032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps = 0\nfor e, epoch in enumerate(range(n_epoch)):\n    progress_bar = qqdm(dataloader)\n    for i, data in enumerate(progress_bar):\n        imgs = data\n        imgs = imgs.cuda()\n\n        bs = imgs.size(0)\n\n        # ============================================\n        #  Train D\n        # ============================================\n        z = Variable(torch.randn(bs, z_dim)).cuda()\n        r_imgs = Variable(imgs).cuda()\n        f_imgs = G(z)\n\n        \"\"\" Medium: Use WGAN Loss. \"\"\"\n        # Label\n        # r_label = torch.ones((bs)).cuda()\n        # f_label = torch.zeros((bs)).cuda()\n\n        # Model forwarding\n        # r_logit = D(r_imgs.detach())\n        # f_logit = D(f_imgs.detach())\n        \n        # Compute the loss for the discriminator.\n        # r_loss = criterion(r_logit, r_label)\n        # f_loss = criterion(f_logit, f_label)\n        # loss_D = (r_loss + f_loss) / 2\n\n        # WGAN Loss\n        loss_D = -torch.mean(D(r_imgs)) + torch.mean(D(f_imgs))\n       \n\n        # Model backwarding\n        D.zero_grad()\n        loss_D.backward()\n\n        # Update the discriminator.\n        opt_D.step()\n\n        \"\"\" Medium: Clip weights of discriminator. \"\"\"\n        for p in D.parameters():\n           p.data.clamp_(-clip_value, clip_value)\n\n        # ============================================\n        #  Train G\n        # ============================================\n        if steps % n_critic == 0:\n            # Generate some fake images.\n            z = Variable(torch.randn(bs, z_dim)).cuda()\n            f_imgs = G(z)\n\n            # Model forwarding\n            f_logit = D(f_imgs)\n            \n            \"\"\" Medium: Use WGAN Loss\"\"\"\n            # Compute the loss for the generator.\n            # loss_G = criterion(f_logit, r_label)\n            # WGAN Loss\n            loss_G = -torch.mean(D(f_imgs))\n\n            # Model backwarding\n            G.zero_grad()\n            loss_G.backward()\n\n            # Update the generator.\n            opt_G.step()\n\n        steps += 1\n        \n        # Set the info of the progress bar\n        #   Note that the value of the GAN loss is not directly related to\n        #   the quality of the generated images.\n        progress_bar.set_infos({\n            'Loss_D': round(loss_D.item(), 4),\n            'Loss_G': round(loss_G.item(), 4),\n            'Epoch': e+1,\n            'Step': steps,\n        })\n\n    G.eval()\n    f_imgs_sample = (G(z_sample).data + 1) / 2.0\n    filename = os.path.join(log_dir, f'Epoch_{epoch+1:03d}.jpg')\n    torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n    print(f' | Save some samples to {filename}.')\n    \n    # Show generated images in the jupyter notebook.\n    grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n    plt.figure(figsize=(10,10))\n    plt.imshow(grid_img.permute(1, 2, 0))\n    plt.show()\n    G.train()\n\n    if (e+1) % 5 == 0 or e == 0:\n        # Save the checkpoints.\n        torch.save(G.state_dict(), os.path.join(ckpt_dir, 'G.pth'))\n        torch.save(D.state_dict(), os.path.join(ckpt_dir, 'D.pth'))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T07:36:23.393118Z","iopub.execute_input":"2022-05-19T07:36:23.393374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}